/* 
 * Copyright (C) 2017 NEC Laboratories America, Inc. ("NECLA"). All rights reserved.
 *
 * This source code is licensed under the license found in the LICENSE file in
 * the root directory of this source tree. An additional grant of patent rights
 * can be found in the PATENTS file in the same directory.
 */
#ifndef DSTORM_HPP_
#define DSTORM_HPP_

#include "dstorm_fwd.hpp"       // fwd decl + a few typedefs + macros
#include "dstorm_msg.hpp"       // NEW: fwd decl class MsgHeader<T>
#include "dstorm_transport.hpp"
#include "ionet/userIoNet.hpp"
#include "ionet/scalIoNet.hpp"

#include <vector>
#include <array>
#include <unordered_map>
#include <signal.h>
#include <assert.h>             // REMOVE !

#if ! defined(WITH_NOTIFYACK)
#if ! WITH_LIBORM
/** \todo NOTIFYACK should be extended to work with ORM (and then with MPI and GPU) compilation.
 * But once we have orm in play we need generic functions to avoid \em all 'transport specific' calls.
 * OMPI Transport should have implementations, and SHM Transport should do nothing.
 * orm does not yet have notify/waitsome and a few other functions.
 * Note: this is also mirrored in dstorm_env.h (autogenerated) */
#define WITH_NOTIFYACK 1
#else
#define WITH_NOTIFYACK 0
#endif
#endif

// forward declarations ------------------------------------
namespace dStorm {

    class Dstorm;               ///< the malt2 communications abstraction
    class ScalGraphs;           ///< local view of the communications graph + utilities

    namespace detail {

        class DstormCommInit;   ///< Dstorm base class to help initialize communications.

#if WITH_NOTIFYACK
        class SegSenderAcks;    ///< old traonsport-based notify-ack synchronization
#endif // WITH_NOTIFYACK

    }//detail::
}//dStorm::
// ---------------------------------------------------------

namespace dStorm {

    typedef mm2::Tnode Tnode;   // mm2 alternative to orm_rank_t

    namespace detail {

        struct DstormCommInit
        {
        private:
            DstormCommInit() = delete;
            DstormCommInit(DstormCommInit const&) = delete;
            /** Common constructor code.
             * When orm is set, invoke orm->proc_init and retrieve copies
             * of \c iProc and \c nProc (o.w. use orm functions directly). */
            void finalize( /*TransportEnum const tr*/ );
        public:
            /** copy orm (it might be \c orm_transport, \c orm_mpi, \c orm_shm or \c orm_gpu in liborm.a)
             * and set state info ptr to NULL. New way allows passing configurables
             * to the transport layer. */
            template< TransportEnum TR >
                DstormCommInit( Transport<TR> const& tr );
            /** distributed/debug \c printf, over chosen Transport, for \e small messages (< 1k chars) */
            int dprintf( const char* fmt, ... );
            /** This function dumps the transport configuration.
             * no-op currently */
            void orm_config_cout();

            /** Deallocate our \c orm copy */
            ~DstormCommInit();
            TransportEnum       const   transport; ///< SHM, MPI, GPU, (see dstorm_fwd.h) allow upcasting orm->obj (if ever necessary) to correct Transport<TR> type.
            orm_rank_t        const   iProc;     ///< idx of current process
            orm_rank_t        const   nProc;     ///< number of process
#if WITH_LIBORM
            struct Orm *        const   orm;       ///< dispatch table to shm/mpi/...
#endif
        };
#if WITH_NOTIFYACK
        /** Track how many out messages we are \c sending with orm_notify,
         * so that we (or user) can \c wait(timeout_ms) (??wait(notifyEnum=NTF_ACK or NTF_DONE)??)
         * to receive back that number of ACKs (along the same set of out-edges).
         *
         * - helps SEGSYNC_NOTIFY_ACK track which notify/ack state.
         *   - used internally in \c Dstorm
         *   - does not help send the acks.
         *
         * - \b ACK sent out means:
         *   - "Your data has arrived, so you can <em>re-use your memory</em> buffer"
         *   - \b Consider an \b ACK2 stage meaning:
         *     - "I am done with my buffer, so feel free to <em>send me the next</em>
         *        one and clobber my buffer"
         *
         * - \b ACK received means "OK to modify send buffer content"
         *   - \b ACK2 (TBD) would mean "OK to send push next buffer"
         *
         * \sa Dstorm::waitAcks(SegNum const s), which should be called by the user
         *     before doing in-place modification of the oBuf of segment \c s.
         *
         * - not thread-safe.
         *
         * \b NOTE: SegSenderAcks does NOT help send back acks. It lacks data
         *          about rbufnums and the ack notification ranges
         *
         * \sa SegInfo::ReduceState for receiver-side helper to count stream DONE (eof)
         */
        class SegSenderAcks {
        public:
            /** We maintain a simple state for each possible \c snd item [0..out_degree-1] in the send list. */
            typedef enum : unsigned char { ACKED, SENDING } Ackstate;

            /** Initialize a sender-side ack helper for a particular segment.
             * \c s SegInfo for this rank, used only during construction.
             * Initially, all items behave as though they have been ACKed.
             */
            SegSenderAcks( SegInfo const& s );
            ~SegSenderAcks() {};

            /** register that we expect an ACK from \c snd 'th sendlist rank */
            void sending( orm_rank_t snd ){
                assert( /*snd >= 0 &&*/ snd <= state.size() );
                assert( state.at(snd) == ACKED );
                if( state[snd] != SENDING ){
                    state[snd] = SENDING;
                    ++nSending;
                    assert( nSending <= state.size() );
                }
            }

            /** await for \b all state[snd]==SENDING to return their ACKs.
             * \return \# of outstanding acks (zero if all acks got back)
             * \throw domain_error for orm error.
             *
             * \post for return value zero, orm_write_notify data has arrived at out-vertex,
             *       so we can modify the send buffer (obuf).  If providing
             *       stronger guarantee for SEGSYNC_NOTIFY_ACK, it is
             *       also safe to Dstorm::push the buffer immediately.
             *
             * \note If we modify and then write again, very quickly, before out-vertex has
             *       read/used the previous data, we could still get mixed-version effects.
             *       So condider an ACK2 meaning out-vertex is "ready for next write".
             *
             * - For \em some algorithms the time to next Dstorm::push() will "always"
             *   be longer than the out-vertex's Dstorm::reduce() time, so ACK2 would
             *   not be useful.
             * - This can provide:
             *   - guaranteed correct xmit (free of 'mixed version' vectors)
             *   - lighter sync than full barrier
             *
             * \todo merge Dstorm::wait into more generic MPI synchronization approach?
             */
            uint32_t wait( orm_timeout_t const timeout = ORM_BLOCK );

            // /** for Stream support -- wait for \b one state[snd]==SENDING to return its ACK. throw on orm error.
            //  * (fine-grained version of \c wait). */
            // void wait( orm_notification_id_t const snd, orm_timeout_t const timeout = ORM_BLOCK );

            /** tick time spent in wait(). use orm_time_ticks to read, and orm_cpu_frequency to convert
             * to seconds (CHECK THIS!).*/
            orm_cycles_t ackTicks() const { return this->t_ack; }

            /** string version of state, 'S' for SENDING, ' ' for ACKED. */
            std::string str();

            //Ackstate const& operator[](orm_notification_id_t const snd) const { return this->state[snd]; }
            //Ackstate      & operator[](orm_notification_id_t const snd)       { return this->state[snd]; }
        private:
            /** In principle, not needed, but here for debug messages during coding. */
            std::vector<Ackstate>   state;
            /** If all is well, only need \c nSending value and wait for that many acks. */
            orm_notification_id_t nSending;
            /// \name values remembered from SegInfo supplied to constructor */
            //@{
            orm_segment_t      const seg_id;
            orm_notification_id_t const ntBeg;    /** notification_id range beginning for NOTIFY */
            orm_notification_id_t const ntNum;    /** notification_id span, same as send-list size */
            //@}
            /** t_notify stats count how long we spent waiting for all [write-]notifications to arrive */
            orm_cycles_t t_notify;
            /** t_ack stats count how long we spent in the ACK \e barrier. orm_time_t is float milliseconds. */
            orm_cycles_t t_ack;
        }; // SegSenderAcks

#endif

    }//detail::

    /** Dstorm has one graph per 'add_segment', with an \c IoNetENUM handle indexing into
     * a \c ScalGraphs vector of graph object implementations.
     *
     * Dstorm will \c push_back a number of standard graphs to prepopulate \c Dstorm::iographs,
     * while use-defined (C++) graphs can be added by calling \c Dstorm::add_ionet.
     *
     * You \c push_back implementations that provide a \c name()
     * function and a \c mkSend() function.  We allow up to 255 graphs.
     *
     * \todo move to detail:: namespace.
     */
    class ScalGraphs {
    public:
        typedef mm2::ScalNet  WrapNet; ///< cache \b node-local send list AND recv list

        /// @name basic construction and send_lists capability
        //@{
        ScalGraphs( Tnode const verts )
            : verts(verts)
              , graphs( /*empty*/ )
        {}
        /** Dstorm \b registers graph types now. \return handle (0,1,2,...).
         * milde_malt will use the handles for Dstorm::send_lists[] and relatives.
         *
         * \p node is \e this \e iProc node
         * \p ptr has an instance of some \c mm2::user::IoNetXXX class, that has:
         *
         * - name()        returning string, possibly long.
         * - shortname()   short and sweeet (use in filenames)
         * - mkSend(node)  vector of destination nodes (sanitized before use)
         *
         * \return IoNet_t handle (e.g. uint_least8_t)
         * \post caller's \c ptr is empty
         * \throw runtime_error if \c ptr is empty or too many graphs
         *                      or wrong-size graph or really bad mkSend function.
         *
         * \sa Dstorm::add_ionet for client version
         */
        IoNet_t push_back( Tnode const node, std::unique_ptr<mm2::UserIoNet>&& ptr );

        /** translate graph number to graph object base class.
         *  returning const would mean can only call const functions.
         *  You must not delete the returned pointer. */
        //mm2::detail::CachingIoNet& operator[]( IoNet_t handle ) const
        WrapNet const& operator[]( IoNet_t handle ) const
        { return *graphs[checkHandle(handle)]; }

        WrapNet & operator[]( IoNet_t handle )
        { return *graphs[checkHandle(handle)]; }

        /** view send_list in graph \c handle for node \c iProc.
         * shorthand for this->operator[](handle)->send(iProc) . XXX remove iPorc argument?
         * \deprecated arg \c iProc for SCALIO==2. */
#ifndef NDEBUG
        std::vector<Tnode> const send( IoNet_t handle, Tnode iProc ){
            assert( iProc == graphs[checkHandle(handle)]->node );
            return graphs[checkHandle(handle)]->send();
        }
#else
        std::vector<Tnode> const send( IoNet_t handle, Tnode /*iProc*/ ){
            return graphs[checkHandle(handle)]->send();
        }
#endif

        /** ScalGraphs optimization return const ref to sendlist of this->node.
         * shorthand for this->operator[](handle)->send() */
        std::vector<Tnode> const& send( IoNet_t handle ){
            return graphs[checkHandle(handle)]->send();
        }

        /** return how many graphs (segments), \b not the number of nodes (verts) */
        IoNet_t size() const { return static_cast<IoNet_t>( graphs.size() ); }
        //@}

    private:
        IoNet_t checkHandle( IoNet_t const handle ) const {
            if( handle >= this->graphs.size() ){
                std::runtime_error e(" graph handle out of range\n");
                //orm_printf( e.what() );
                throw e;
            }
            return handle;
        }
    private: // since it is const, may as well publicize...
        Tnode const verts;  ///< all graphs given to us must have this number of vertices

    private:
        /** implemented as vector of owned ptrs to impls of abstract ScalIoNet. */
        std::vector< WrapNet* > graphs;
    }; // class ScalGraphs

    class Dstorm : public detail::DstormCommInit    ///< pre-initialize iProc, nProc
    {
    public: // types
        typedef ScalGraphs Graphs;
        typedef std::unordered_map< SegNum, SegInfo* > SegInfoMapType;
#if WITH_NOTIFYACK
        typedef std::unordered_map< SegNum, dStorm::detail::SegSenderAcks* > SegAcksMapType;
#endif
        // TODO: uint32_t **SHOULD** really be unsigned char, or some
        //       much smaller "SegIdx" type.  orm_segment_t ???

    private:
        /** add_segment produces real objects, whose pointers-to-base
         * need to be maintained.  Member functions foo<FMT> unambiguously
         * upcast to FMT user implementation handles the segment. */
        SegInfoMapType seginfos;
#if WITH_NOTIFYACK
        SegAcksMapType segacks; ///< helper for SEGSYNC_NOTIFY_ACK
#endif

    public:
        /**Now supports configurables for the transport layer.
         * Transport<OMPI>() and Transport<SHM>() can be used in
         * the Dstorm constructor to get "default" settings for these transport
         * layers.
         *
         * NEW: Dstorm constructor does NOT finish with a net->barrier() call
         *      (changed to ease thread creation for SHM transport)
         */
        template< TransportEnum TR >
            Dstorm( Transport<TR> const& tr );

#if WITH_SHM
        /** default constructible for compilation tests or quick-tests.
         * This is a C++11 "delegating constructor", defaulting to SHM
         * transport with \c Transport<SHM>::default_nThreads threads. */
        Dstorm() : Dstorm( Transport<SHM>() ) {}
#endif

    private:
        void init_common();                 ///< common cosntructor code
    public:

        ~Dstorm();

        /// @name basic utilities
        //@{
        int get_iProc();    ///< rank of this orm process, in 0..get_nProc()-1
        int get_nProc();    ///< total \# of orm processes
        /** add a segment of type FMT with room for \c cnt data items
         * and with communication pattern \c ionet.
         *
         * Sets internal machinery so that \c s becomes a [unique] handle for
         * operations (store,push,reduce,...) involving this segment.
         *
         * \tparam args are forwarded to the SegImpl<FMT> \b constructor
         *
         * \tparam FMT is a high-level implementation such as Seg_VecDense,
         *             the example supplied with libdstorm, or SegSVector,
         *             an example from Leon-GPL/ showing how you might code
         *             your own "vector data" format. TBD: milde FMTs such
         *             as Seg_Tensor, ...
         * \tparam Args... [optional] FMT::type segment constructor parms
         *                 (expect most types will have no optional Arglist)
         * \p s      a unique handle you will use to refer to this segment of type FMT
         *           Ex. You might want 20 \c FMT=seg::VecDense vectors in
         *           your neural network application, indexed by s=0..19.
         *           \c s values are arbitrary values assigned by the user.
         *           \pre \c s is not currently assigned to another segment
         * \p ionet   communication graph (esp. "send" edges for push). Some
         *            ionet handles are pre-defined;
         *            others, you can add with \c add_ionet(..)
         * \p policy  \c SegPolicy layout and communication protocol options
         * \p cnt     how many items must fit into any buffer of the segment.
         *            Sets capacity of each buffer within segment \c s,
         *            in units of the data type of FMT
         *            (some Tdata type, \ref dStorm::detail::SegBase< IMPL, TDATA >).
         *
         * \post \c {store/push/reduce} operations enabled on this node.
         *
         * \return nothing for now
         * \throw ...?
         *
         * - Basic operation:
         * 1. Constructs SegImpl<FMT> -> Impl(argument pack) -> ... SegBase<FMT,TDATA> -> SegInfo
         * 2. Initialize the "fresh" segment for orm use.
         *   - You should assume the segment content to be uninitialized, although
         *   - rank zero, or some verbose settings for debug **may** zero-initialize
         *     to cut down on valgrind messages.
         *   - \c store is assumed to populate oBuf vector,
         *   - \c reduce is assumed to set the iBuf vector, and
         *   - \c push (from other ranks) sets the rBuf vector[s].
         *
         * - There is an internal barrier \em before returning.
         * - NOTE: \c add_segment used to be const, but for less typing,
         *         seginfos is now kept inside the Dstorm object.
         *
         * \sa SegInfo
         */
        template< typename FMT, typename... Args >
            void add_segment( SegNum const s,
                              IoNet_t const ionet,
                              SegPolicy const policy,
                              uint32_t const cnt,
                              Args&&... args );
#if 0
        /** \c add_segement can be construct a SegImpl<FMT> from a MsgHeader<FMT>
         * corresponding to a full view of the object.  This saves supplying arguments
         * as template parameters, and simplifies the calling sequence. */
        template< typename FMT >
            void add_segment( SegNum const s,
                              IoNet_t const ionet,
                              MsgHeader<FMT> const& hdr    ///< "maxview" dimensionality
                            );
#endif

        /** delete segment that was added via \c add_segment( SegTag<FMT> ).
         * Note: maybe we will always delete ALL segments of given FMT?
         */
        void delete_segment( SegNum const s );

        /** const SegInfo accessor.
         * \tparam FMT is the client's segment id (unsigned)
         * \return ref if segment valid, else throws runtime_error
         * \pre add_segment<FMT>(...) has been called
         * \post client abandons returned ref before any call
         *       to \c delete_segment.
         * \throw runtime_error if \c s not in \c seginfos
         */
        SegInfo const& getSegInfo( SegNum const s ) const;

        /** Register user-defined IoNets with Dstorm.
         *
         * Passes the UserIoNet to this->iographs (ScalGraphs) set of iographs.
         * \returns the index (handle) of the new graph.
         * Builtin graphs reserve \c OldIoNetEnum tag values
         * 0..IONET_MAX, and user handles will get back values > IONET_MAX (>=?)
         */
        IoNet_t add_ionet( std::unique_ptr<mm2::UserIoNet>&& ptr );

        //@}

        /// @name for distributed calculation
        //@{
        /** store \c cnt items beginning at \c offset of \c iter.
         * throw if iter non-const or deref incompatible with Tdata of FMT
         * \tparam FMT segment type, \c is_segment_format<FMT>::value==true
         * \p s      which segment (a SegInfo of derived type SegImpl<FMT>).
         * \p iter   ForwardIterator pointing to zero'th item
         * \p cnt    how many items to store...
         * \p offset ... after advancing \c iter by \c cnt items
         * \p weight [default=1.0] we can send a scalar weight, along with the vector.
         *           This weight can be stored in the header, and allows us to easily
         *           support push-sum style algorithms.
         *
         * - If not all items fit into buffer...
         *   - \b OLD: silently ignored extra items (no error)
         *   - \b NEW: throw std::length_error
         *
         * - \c store prepares \c s's oBuf for a \c push to all nodes in this
         *   rank's \c getSendVec(s)
         * - It may copy data and change data type
         * - Or, if the iterator is a pointer with address and type compatible
         *   with \c ptrOdata(s), the data copying may be elided
         *
         * - Cannot \em easily remove the FMT here, because virtual
         *   template functions are NOT possible in C++.
         * - Other API possibilities:
         *   - Mirror std::copy and std::copy_if mechanism, but "know" that the
         *     output iterator is to the ptrOdata region.
         *   - Provide a return value to indicate
         *     1. store<FMT>( begin, end )
         *     2. store<FMT>( begin, end, predicate )
         */
        //@}
        template< typename FMT, typename IN_CONST_ITER >
            void store( SegNum const s,
                        IN_CONST_ITER iter, uint32_t const cnt,
                        uint32_t const offset=0U, double const weight=1.0 ) const;
        /** store: begin,end iterator version.
         * For now, just uses std::distance and shunts to iter + cnt code.
         * (Simplifies adding new user-defined vector formats)
         */
        template< typename FMT, typename IN_CONST_ITER >
            void store( SegNum const s,
                        IN_CONST_ITER iter, IN_CONST_ITER const end,
                        uint32_t const offset=0U, double const weight=1.0 ) const;
#if 0
        /** begin,end iterator version */
        template< typename FMT >
            void store( SegNum const s,
                        typename FMT::Base::TDATA const* iter, typename FMT::Base::TDATA const *end, uint32_t offset=0U ) const;
#endif

        /** send out the node's output buffer within the segment to other ranks.
         * \c add_segment defines who gets content from this node.
         * \pre assume \c store was previously called to load data
         * (or the user has somehow directly manipulated a la HogWild)
         *
         * \return -nErr (if any), o.w. \# bytes succesfully pushed
         *
         * \todo Dstorm::push might take args for:
         * (a) ionet subgraph
         * (b) dynamic adjustment of number of sends
         *     - bandwidth related?
         *     - randomized choose fraction f from send list?
         *     - choose first fraction f of send list?
         */
        ssize_t push( SegNum const s ); // non-const (build_lists() error recovery)
#if WITH_NOTIFYACK
        /** Extended push functionality for NTF_SELECT, NTF_DONE, NTF_RUNNING (add more
         * as required)
         *
         * - \p s is a valid segment handle
         * - \p done may be:
         *   - \b NTF_DONE to initiate a REDUCE_STREAM eof handshake,
         *   - \b NEW: or \b NTF_SELECT to send out obuf
         *   - \b NTF_RUNNING to reset a REDUCE_STREAM receiver's stream state
         * - \p snd if supplied, send \c NTF_DONE, or obuf for just one of the sendlist streams.
         *   - If absent (or -1U), initiate eof handshake for all sendlist destinations.
         *   - ... or, if NTF_SELECT, send obuf via one (or all) out-edges.
         *
         * NTF_SELECT (new):
         * - NTF_SELECT modifies the default \c push(SegNum) semantics of <em>push via <b>all</b> out-edges</em>,
         *   - and means obuf is written \b only to rank sendlist[snd],
         *   - or, if \c snd is -1U or absent, via \em all out-edges.
         * - NTF_SELECT is available for *any* SegPolicy,
         *   - regardless of whether write notifications are used for the SegPolicy
         * - Observe that the usual use case, \c push(SegNum), is \em equivalent to push(SegNum,NTF_SELECT[,-1])
         *
         * NTF_DONE usage:
         * - 1. wait for any previous acks to be returned (wait(SegNum,orm_timeout_t ms))
         * - 2. initiate NTF_DONE handshake (no data is transfered).
         *   - This uses orm_notify, so it is not supported WITH_MPI (or WITH_GPU).
         * - client should \c wait(SegNum,orm_timeout_t ms) for all NTF_DONE acks to return
         *   before deleting the segment.
         *
         * NTF_SELECT usage:
         * - compatible with any SegPolicy. Notify-ack machinery should still work if you want it.
         * - Example:
         *   - REDUCE_STREAM | SEGSYNC_NOTIFY_ACK | SEG_ONE in conjunction with STREAM IoNet_t
         *   - would be able to send one training example to a single one of the available destinations.
         *     - ... such as \c mm2::user::IoNetStream <em>one-to-all-others</em> IoNet.
         *   - \sa dstsgd6.cpp
         *
         * NTF_RUNNING usage:
         * - sender:
         *   - write loop: (store,push,wait)
         *   - push(SegNum,NTF_DONE), wait(SegNum)
         *     - ... barrier + delete_segment
         *     - or ... Another write loop 
         * - reducer:
         *   - begins polling Dstorm::reduce for items during first stream operation.
         *   - Dstorm::reduce returns -1U when all in-edges have send NTF_DONE
         *     - ... barrier + delete_segment
         *     - or ... Dstorm::push(SegNum,NTF_RUNNING[,which]) to reset stream to
         *       clear out the end-of-stream flags (and accept new streams)
         *       [default which = on all in-edges].
         *       - \return zero for push(SegNum,NTF_RUNNING)
         */
        ssize_t push( SegNum const s, NotifyEnum const done, uint32_t const snd=-1U );
#endif
        /** reduce all new inputs from recv_list nodes into this node's
         * "iBuf" buffer for segment \c seg_idx.
         *
         * This operation is a buffers-to-buffer operation within the
         * segment <em>for now</em>.
         * Later, may need user-defined reduce operations (e.g. to supply
         * custom weightings to different inputs, perhaps given a list
         * of "freshly available" input buffer numbers for the segment?).
         * For some REDUCE_* SegInfo policies, iBuf==oBuf (no distinct iBuf).
         *
         * All cnt, offset homogenous is easy case. Inhomogenous case
         * will throw if reduce output doesn't fit in reduce buffer of
         * segment.
         *
         * \return number of inputs that were available and used to
         * form the output vector.
         *
         * \return -1U for SegPolicy REDUCE_STREAM to signal end of read stream.
         *
         * \b SegPolicy notes
         *
         * - SEGSYNC_NOTIFY[_ACK]
         *   -side effect of reduce is to wait for notifies that all in-edge data has arrived.
         * - SEG_ONE:
         *   - calling \c reduce is now allowed for REDUCE_NOP,
         *     because it may have a <em>synchronisation side-effect</em>,
         *     of waiting for NOTIFY (or NOTIFY_ACK) that data has arrived
         *     from all in-edges.
         *     - in-degree should be one?
         * - REDUCE_STREAM:
         *   - On the sender side, NTF_DONE handshakes are done via
         *     \c push(SegNum,NTF_DONE [,sndlist_number] ).
         *   - Implies end-of-stream -- no more input need be accepted.
         *   - If reduce is called in a thread, reduction thread should exit.
         *   - When all (if any) reduce threads are joined, then can delete_segment.
         *
         * There is still a question of what GRADIENT=0.0 means
         *
         * 1. It could mean "no information" (sparse gradient),
         *    in which case reduce( {4,0,6}, {0,2,8} ) would be {4,2,7} <-- sum
         *    I.e., every value in sum gets divided by the number of nonzero items
         *    (1 or 2 in this example)
         * 2. Or it could mean "gradient really is 0.0",
         *    in which case reduce( {4,0,6}, {0,2,8} ) would be {2,1,7} <-- avg
         *    I.e., every value in sum gets divided by the number of items
         *    (2, a constant, in this example)
         *
         * For now, we assume case 2, which is the easiest to program.
         * Every value in the sum gets divided by a constant.
         *
         */
        uint32_t reduce( SegNum const s ) const;
#if 0
        /** reduce directly into use-supplied buffer, as an example
         * for dense vectors. */
#endif
#if 0
        /** lower-level reduce returns pairs of iterators, as an example
         * for spare vectors. NOTE: simd-friendly sparse formats should
         * be investigated. */
#endif
#if 0
        /** A more generic reduce suppors arbitrary movement of
         * reduction result to an output iterator. */
        template<typename FMT, typename OITER>
            void ( OITER oIter );
#endif
        /** barrier across all ranks.
         * \p timeout_ms \b NEW: orm will hang ALL processes if using ORM_BLOCK,
         *               so use a default timeout of 1 minutes.
         *
         * \todo recoverable barrier? Can we reinitialize the network somewhat
         * transparently?
         *
         * \b NEW: \throw Dstorm::ErrNetwork in case of timeout. Timeout
         * setting is \c get_default_barrier_timeout_ms(), and defaults
         * to \c DSTORM_BARRIER_TIMEOUT_MS (perhaps 15 minutes).
         *
         * - With all processes stuck at barrier, CPU usage is still 100%.
         * - Improves case where everybody hits this exception and all ranks
         *   are at least able to terminate.
         * - Catch clause can do some \e checkpointing, to preserve model.
         */
        void barrier( orm_timeout_t const timeout_ms ) const;
        /** using \c this->barrier_timeout_ms */
        void barrier() const {
            this->barrier( this->barrier_timeout_ms );
        }

        /** modify barrier timeout from DSTORM_BARRIER_TIMEOUT_MS */
        void set_default_barrier_timeout_ms( orm_timeout_t const ms ){
            this->barrier_timeout_ms = ms;
        }
        /** get barrier default timeout */
        orm_timeout_t get_default_barrier_timeout_ms() const {
            return this->barrier_timeout_ms;
        }

        /// @name special utilities
    public:
        /** Install a user-defined function into segment \c s that has REDUCE_STREAM policy.
         *
         * - REDUCE_STREAM implies SEGSYNC_NOTIFY_ACK.
         * - REDUCE_STREAM will call \c userfunc during \c reduce
         *   for each available rbuf as it is found,
         *   - after which it will send back the ACK.
         *   - this ACK should have \em strong semantics (OK to write to me again)
         *     - as opposed to weaker semantics (your data arrived)
         *   - This strong ACK guarantees userfunc never sees 'mixed-version' data
         * - REDUCE_STREAM will not block waiting for any rbufs to be available
         *   - as opposed REDUCE_AVG_RBUF_* for which NOTIFY_ACK implies
         *     waiting for \em all rbufs to arrive.
         *
         * \throw for invalid seg
         * \return old userfunc pointer (likely the original nullptr)
         */
        SegStreamFunc setStreamFunc( SegNum const s, SegStreamFunc const userfunc );
        orm_rank_t nDeadCount() const; ///< liveness getter, force a re-count.
        orm_rank_t nDead() const;      ///< liveness getter, a bit faster
        //@{
    public:
#if !WITH_GPU && WITH_NOTIFYACK
        /** For NOTIFY_ACK segments, client can explicitly block on receipt of acks.
         * If client does not, dstorm will do it before the next push, for sure.
         * (should it be in \c store(SegNum...)?)
         *
         * \return outstanding acks, zero means \e all previous \c Dstorm::push have completed.
         *
         * - ORM_TIMEOUT is \e not considered an error.
         * - \e possible Weak Semantics: data has been received
         * - \e actual Strong Semantics: data has been received \e and full processed by
         *   the destination rank's \c reduce operation.
         * - This can be used to guarantee zero mixed-version vectors for reduce, which
         *   is a stronger guarantee than \c barrier() provides!
         * - for in-place modification of obuf vector data, nice clients will \c wait
         *   before modifying the obuf content.
         *   - Ex. if fprop and bprop are some read-only obuf operations, then
         *     \c store... \c push... fprop .. bprop .. \b wait... adjust...
         *     is a "nice" sender loop. (Note: \c reduce operation not shown, because
         *     sometimes it is an obuf-modifying operation, sometimes not).
         *
         * This calls SegSenderAcks::wait for segment \c s
         * \throw std::domain_error if \c segPolicy is not SEGSYNC_NOTIFY_ACK
         *
         * \sa ackTicks for reading how many ticks was spend waiting for acks
         */
        uint32_t wait( SegNum const s, orm_timeout_t const timeout_ms );
#endif
#if WITH_NOTIFYACK 
        /** return cumulative ticks spent waiting for acks in \c wait(s,timeout_ms).
         * Without notify-ack support, just return orm instantaneous ticks (\e not cumulative!)
         * \return 0 if NOTIFY_ACK is unsupported (~ liborm has not implemented it yet).
         * \throw if NOTIFY_ACK is supported, but segment \c s does not use acks
         *        (maybe change to return 0?).
         */
        orm_cycles_t ackTicks( SegNum const s );
#endif

        /** orm network error, likely unrecoverable (perhaps dead node?) */
        class ErrNetwork : public std::runtime_error {
        public:
            ErrNetwork( std::string const& what_arg ) : std::runtime_error(what_arg) {}
            ErrNetwork( char const* what_arg ) : std::runtime_error(what_arg) {}
            virtual ~ErrNetwork() {}
            //orm_return_t error; // ?
        };

        /** Explicitly force degrees of sync / NIC queue emptiness.
         * Hopefully you use this mainly for debugging.
         * \c push and \c reduce use queue 0. may throw orm_error */
        void wait( Qpause const             qpause,
                   orm_queue_id_t const   queue_id,
                   orm_timeout_t const    timeout_ms = ORM_BLOCK );

        /** return this rank's local send/recv list. \pre add_segment<FMT>
         * has successfully set up FMT as a valid segment. */
        std::vector<Tnode> const & netSendVec( IoNet_t const ionet ) const;
        std::vector<Tnode> const & netRecvVec( IoNet_t const ionet ) const;

        /** retrieve send/recv_list specific to this iProc'th node.
         * Within each segment of \c segInfoMap[SegIdx], i/o buffers
         * map sequentially to elements of send/recv_lists
         * These lists never reflect nodes current alive/dead status.
         *
         * TODO: review ** ALL ** usages of send/recv lists for whether
         *       they include live nodes only.  Segment buffer numbers
         *       ** MUST ** refer to all possible nodes (even dead ones),
         *       so it may be that \c get_send_list is actually better
         *       off being as large as possible -- it is easy to check
         *       liveness as needed.
         */
        std::vector<Tnode> const& segSendVec(SegNum seg) const;
        std::vector<Tnode> const& segRecvVec(SegNum seg) const;

        //@}

    public:
        /// \name debug
        //@{
        //void print_seg_info( SegNum const s );
        //void print_sync_list(IoNet_t const sync_model);
        void            print_seg_info(SegNum seg_idx) const; ///< throw if seg_idx invalid
        std::string     name( IoNet_t const& sync_model ) const;
        void            print_local_seg(int);
        void            print_all_local_segs(std::ofstream &);
        void            print_status(void);
        std::string     print_sync_list(IoNet_t sync_model);
        mm2::ScalNet const& getScalNet( IoNet_t sync_model ) const
        { return this->iographs[sync_model]; }
        //@}

        /// \name Buffer pointer getters
        /// shunts to functions in \c SegInfo hierarchy
        /** return pointer to data of various buffer types within segment.
         *
         * - If the segment is for vector data, this points to the raw data
         *   (after any vector headers) (historical convention).
         * - These are hopefully only for debug/test
         *
         * For client \em segment implementations, please use \c SegBase<...>::buf(s)
         * or \c SegBase<...>::data(s) (do not need to go via SegInfo's Dstorm ptr).
         *
         * Are these even useful? perhaps privatize!
         */
        //@{
        // ??? protected:
        /** ptr to \c store / \c push vector data of this (iProc) node. */
        orm_pointer_t ptrOdata( SegNum const s ) const;
        /** ptr to input vector data, set by \c reduce function. */
        orm_pointer_t ptrIdata( SegNum const s ) const;
        /** ptr to data segment of an IoNet input buffer */
        orm_pointer_t ptrRdata( SegNum const s, orm_rank_t const recv_index ) const;
        //@}

    private:
        /// \name private helpers
        //@{
#if 0
        /** For \b test purposes, return ptrs to \em every available
         * buffer in a segment. This may be of little use if the segment's
         * IoNet_t represents a complicated communication pattern, and
         * more segment internals are not exposed/documented.
         *
         * \deprecated
         */
        std::vector<float *> get_dense_copy_ptrs(int seg_idx);
        // UNUSED: std::vector<struct vec_header *>  get_sparse_copy_ptrs(int seg_idx);
#endif

        //void            clear_seg(int);
        //void            clear_all_segs();
        void        build_lists();  ///< track "extra" info about in-edge sources and out-edge destinations
        void        netRecover();   ///< recover from node failure (not well tested)
        /** funnel push(SegNum) and push(SegNum,NTF_SELECT,which) through common code.
         * This selects either \c push_impl_host,
         * or, if WITH_GPU and using Transport<GPU>, \c push_impl_gpu. */
        ssize_t     push_impl( SegNum const s, uint32_t const which );
        /** cpu implementation of \em push operation */
        ssize_t     push_impl_host( SegNum const s, uint32_t const which );
#if WITH_GPU
        /** special version for Transport<GPU>. */
        ssize_t     push_impl_gpu( SegNum const s, uint32_t const which );
#endif
#if 0
        /** Where should we send back an ack-notify?
         * \p g     for graph g,
         * \p ircv  indicates an in-edge \e sender, at recvlist[ircv] (ie rank iographs[g].recv()[ircv])
         *
         * - We look up \c SenderInfo si for \e sender,
         * - skip si.sendlist_size slots that are used for write-notify,
         * - and then skip si.sendlist_index more,
         *   - wheresi.sendlist_index is our position in the sendlist of \e sender.
         */
        orm_notification_id_t getAckNid( IoNet_t const ionet, uint32_t const ircv );
#endif
        //@}


    private:
        /** private access to existing seginfos[s] \b and marked valid.
         * \throw runtime_error unless \c s in \c seginfos \e and marked \em active.
         */
        SegInfo & validSegInfo( SegNum const s, char const* funcname="" ) const;
        /** helper for Dstorm::reduce of REDUCE_STREAM segments.
         * \return true if all recv streams have gotten their NTF_DONE eof signals.
         * helps \c reduce(SegNum) return special -1U value when all streams are NTF_DONE. */
        static bool streamCheckEof( SegInfo const& sInfo );
#if WITH_NOTIFYACK
        /** look up the SegSenderAcks object.
         * \throw if it does not exist (domain_error or runtime_error) */
        detail::SegSenderAcks& validSegSenderAcks( SegNum const s ) const;
#endif
    private:
        /// @name main process and network definitions
        //@{
        /** send/rece lists (adjacency list of network, per segment)
         * AND(oops) node status (dead list) */
        detail::DstormLiveness *net;
        /** need some coordination between ranks on \c same node.
         * 
         * - Introduced as a IB debug measure but \em should be unnecessary
         *   if orm calls are always from a unique thread (of each process).
         *   - is set and possibly locked for Transport<IB>.
         *   - is set but never locked for Transport<OMPI>
         *   - unused for Transport<GPU>,
        */
        detail::DstormIPC *ipc;

        friend class SegInfo;
        /** All graphs now get \em registered.
         * Several graphs are pre-registered for compatibility with
         * historical milde_malt \c OldIoNetENUM values. */
        Graphs iographs;
        // Note: unported graph types will have illegal handle value 255 or IONET_MAX
        //       and will throw some error if you try to use them.
        // loops "over all graph types" should iterate from IoNet_t 0 to iographs.size();
        //@}

        /** For write from rank \f$src\f$ to the \f$i\f$'th destination \f$dst=src:\mathrm{send}_G[i]\f$
         * \c send_bufnums[G][i] is the index of \f$src\f$ in the recv_list
         * of destination node send_lists[G][i].
         *
         * - For graph G, \f$ src = dst:\mathrm{recv}_G\left[ src:\mathrm{send\_bufnums}_G[i] \right]\f$
         *   - means \c send_bufnums[G][i] is a <em>0,1,... offset "r" at destination</em>
         *     <tt>iographs[G].send()[i]</tt> <em>reserved for our write</em>, or notification.
         * - I.e., the "r" s.t. for dst = iProc:<tt>send_list[i]</tt>,
         *   dst:<tt>recv_list[r] == iProc</tt>.
         *   - For a SEG_FULL segment, this "r" determines the segment buffer at the destination
         *     to which we \c Dstorm::push(*) data, and which notification we may trigger.
         *
         * - At any node:
         *   - send_lists[G] size is out-degree of graph.
         *   - recv_lists[G] size is in-degree of graph.
         *   - send_bufnums[G] size is same as send_lists[G] size.
         *     - buffer number to send to is \c recvBeg()+send_bufnums value.
         *   - send_bufnums[G] reflects send and receive list information \b only.
         *     - it retains its meaning independent of a segment's \c SegPolicy.
         *
         * \sa segSendVec() for our send_list (prioritized) (our send_lists[G] is now \c iographs[G].send())
         * \sa segRecvVec() for our recv_list (our recv_lists[G] is now \c iographs[G].recv())
         * \sa recv_src, for a mechanism to determine an offset reserved for us to return an
         *     ACK to (or read from) a node within our recv_list.
         *
         * \todo vector<vector<>>  --- support user graphs!
         */
        std::unordered_map<IoNet_t,std::vector<orm_rank_t>> send_bufnums;

        /** We need to know a bit about every \e sender node in our receive list (our in-edges). */
        struct SenderInfo {
            orm_rank_t sendlist_index;    ///< where \c iProc is in the sendlist of \e sender
            orm_rank_t sendlist_size;     ///< out-degree of \e sender
            orm_rank_t recvlist_size;     ///< in-degree of \e sender
        };
        /** For ACKs along the reverse path of sends from remote \e src \c iographs[G].recv()[i]
         * to \e iProc (this node), we maintain \c recv_src[G][i].sendlist_index as index of node
         * \e iProc in the send_list of \e src.
         *
         * - I.e., the 0,1,... offset "s" s.t. for writer rank writ = iProc:<tt>recv_list[i]</tt>, writ:<tt>send_list[s]</tt> == iProc.
         *   - or \f$ src = writ:\mathrm{send}_G[s] = writ:\mathrm{send}_G\left[ src:\mathrm{recv\_bufnums}_G[i] \right]\f$
         * - This "s" determines for each of our receive-buffer data notifications,
         *   <em>which notification_id at data source iProc:recv_list[i] should
         *   receive the orm_notify with the ACK signale</em>.
         *   - perhaps we will also use it to implement segment read behaviors, later?
         * - \c recv_src[G] size is same as our \c recv_lists[s] size, and the same as the
         *   number of rBufs in a SEG_FULL segment).
         *
         * - ACK Summary:
         *   - OK, so iProc will send an ACK back to rank sender = iographs[G].recv()[i].
         *     -  s.t. iProc == sender:iographs[G].send()[ s ],
         *     -  where s = iProc:recv_src[G][i] is \c SenderInfo::sendlist_index.
         *   - So... what notification_id (NID) is this?
         *     - 0..SenderInfo::recvlist_size (if any) are reserved for sender's NOTIFY signals
         *     - next SenderInfo::sendlist_size (if any) are reserved for ACKs
         *       - we get one of these ACK slots:
         *       - ACK to NID = <B>sender.recvlist_size + sender.sendlist_index</B> on rank <B>'sender'</B>
         */
        std::unordered_map<IoNet_t,std::vector<SenderInfo>> recv_src;

        /** default \c barrier() timeout in ms. 
         * \sa dstorm_fwd.hpp for \c DSTORM_BARRIER_TIMEOUT_MS (perhaps 15 minutes?).
         * \sa get_default_barrier_timeout_ms \sa set_default_barrier_timeout_ms */
        orm_timeout_t barrier_timeout_ms;

        struct sigaction maltaction;

    }; //class Dstorm

}//dStorm::
#endif // DSTORM_HPP_
